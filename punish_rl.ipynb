{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30c0ae1",
   "metadata": {},
   "source": [
    "# Playing Punish with Reinforcement Learning\n",
    "### Jim Shepich III\n",
    "### Updated: 8 October 2022\n",
    "### Time Required: 15 hours 27 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ee6cc",
   "metadata": {},
   "source": [
    "# Contents <a id=\"contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c231906",
   "metadata": {},
   "source": [
    "- [Import Packages](#import-packages)\n",
    "- [Notebook Settings](#config)\n",
    "- [Representing Game States](#states)\n",
    "- [Representing Actions](#actions)\n",
    "- [Simulating the Game Environment](#environment)\n",
    "- [Enumerating State Space](#state-space)\n",
    "- [Modeling Enemy Strategies](#enemy-strategies)\n",
    "    - [Possible Actions Reweighted by Limited Empirical Strategic Samples (PARLESS)](#parless)\n",
    "    - [PARLESS Augmented With Neural Networks (PAWNN)](#pawnn)\n",
    "- [Enumerating State-Action Transition Probabilities](#transitions)\n",
    "- [Value Iteration](#value-iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca0a36",
   "metadata": {},
   "source": [
    "# Import Packages <a name=\"import-packages\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Combinatorics\n",
    "using StatsBase\n",
    "using Random\n",
    "using JSON\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d84af6",
   "metadata": {},
   "source": [
    "# Notebook Settings <a name=\"config\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1903c",
   "metadata": {},
   "source": [
    "This section allows a user to configure the setttings of this notebook. For the most part, it will allow you to toggle between generating results or loading them from a file on disk, as well as set model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28783467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{String}} with 3 entries:\n",
       "  \"value-iteration\"          => Dict{String, Dict{String, Integer}}(\"naive\"=>Di…\n",
       "  \"transition-probabilities\" => Dict{String, Dict{String, Any}}(\"naive\"=>Dict(\"…\n",
       "  \"state-space\"              => Dict{String, Any}(\"generate\"=>false, \"filepath\"…"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = Dict(\n",
    "    \"state-space\" => Dict(\n",
    "        \"generate\" => false,\n",
    "        \"filepath\" => \"statespace.json\"\n",
    "    ),\n",
    "    \"transition-probabilities\" => Dict(\n",
    "        \"naive\" => Dict(\"generate\"=>false,\"filepath\"=>\"naive_transitions.json\"),\n",
    "        \"parless\" => Dict(\"generate\"=>true,\"filepath\"=>\"parless_transitions.json\"),\n",
    "        \"pawnn\" => Dict(\"generate\"=>true,\"filepath\"=>\"pawnn_transitions.json\")\n",
    "    ),\n",
    "    \"value-iteration\" => Dict(\n",
    "        \"naive\" => Dict(\"generate\"=>true,\"episodes\"=>10)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939dce8",
   "metadata": {},
   "source": [
    "JSON only allows keys to be strings. This is a bit of a problem, since our transition probability maps and state-action value functions are nested dictionaries with integer keys. Rather than converting encoded states and actions to strings when performing lookups, which will be an inefficiency that really adds up, we'll just create a recursive function to parse the keys of these JSON-serialized dictionaries as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0396dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "strkeys2int (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function strkeys2int(d::Dict{String,Any})\n",
    "    \"\"\"This is a function that recursively converts the keys of a `Dict{String,Any}`\n",
    "    and its nested `Dict{String,Any}` values into `Int64` data.\"\"\"\n",
    "    return Dict(parse(Int64,key)=>strkeys2int(value) for (key,value) in d)\n",
    "    #Parse all keys as integers and then call this function on the value.\n",
    "    #If the value is a `Dict{String,Any}`, then its keys will be converted too;\n",
    "    #if not, the value will be left alone.\n",
    "end\n",
    "\n",
    "function strkeys2int(value::Any)\n",
    "    return value\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455169e",
   "metadata": {},
   "source": [
    "# Representing Game States <a id=\"states\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a5bd5",
   "metadata": {},
   "source": [
    "In a standard, two-player game of Punish, the game state consists of the following attributes:\n",
    "\n",
    "- The cards in the two players' hands\n",
    "- The cards that are showing (cards that have been played and the two cards that have been revealed)\n",
    "- The cards that are face-down in the deck (which can potentially be played via the Feint action)\n",
    "- Whether or not each player has already feinted\n",
    "- Whether or not each player is exhausted from using the Punish action\n",
    "- Both players' HP totals\n",
    "- The breath number \n",
    "\n",
    "Although the measure number is also an aspect of the game state, we will exclude it from our modeling because although measure number may have an impact on how a player's attitudes toward taking risks evolves, it does not change the mechanics of the game. Because a game of Punish can theoretically last for an indefinite number of measures (much like Rock-Paper-Scissors), it is necessary that we exclude measure number in order to have a finite state space.\n",
    "\n",
    "We will design our model to support an agent with the limited perspective of a single player. This means that instead of seeing the specific cards in the opponent's hand and the deck, the agent will only be able to see the number of cards in each. Additionally, since the number of face-down cards in the deck can be determined based on feinting status, we don't need to include it in the state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9df3c",
   "metadata": {},
   "source": [
    "In the next cell, we create a custom structure in which to store a game state. For convenience, we will use `NamedTuple` structures to store the number of each card type in the player's hand, the player's status conditions, enemy's status conditions and hand size, and number of each card type in the discard pile. \n",
    "\n",
    "Victory and loss states will be singleton structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7126d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type PunishState end\n",
    "\n",
    "struct DuelingState <: PunishState\n",
    "    breath::Int64\n",
    "    hand::NamedTuple{(:guard, :rush, :dodge, :strike, :punish), NTuple{5, Int64}}\n",
    "    status::NamedTuple{(:hp, :exhausted, :feinted), Tuple{Int64, Bool, Bool}}\n",
    "    enemy::NamedTuple{(:hp, :hand_size, :exhausted, :feinted), Tuple{Int64, Int64, Bool, Bool}}\n",
    "    discard::NamedTuple{(:guard, :rush, :dodge, :strike, :punish), NTuple{5, Int64}}\n",
    "end \n",
    "\n",
    "struct WinState <: PunishState end\n",
    "struct LossState <: PunishState end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bb09e",
   "metadata": {},
   "source": [
    "Although a struct with `NamedTuple` fields is convenient to work with, it is not a memory-efficient data structure to use in the state-action value function, which we will represent with a lookup table. So, in the next cell, we will implement functions to encode the game state as an integer and decode a game state from such an integer. We will encode a win as -1 and a loss as -2; all dueling states will be encoded as positive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a529743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode_state (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode_state(state::DuelingState)\n",
    "    \"\"\"This function encodes a `DuelingState` structure as an integer in which each digit\n",
    "    corresponds to an attribute of the game state.\"\"\"\n",
    "    digit_list = vcat(\n",
    "        [state.breath],\n",
    "        [card for card in state.hand],\n",
    "        [Int(stat) for stat in state.status],\n",
    "        [Int(stat) for stat in state.enemy],\n",
    "        [card for card in state.discard]\n",
    "    )\n",
    "    #Sequentially add each attribute of the game state to a list of digits.\n",
    "    return sum([digit*10^(i-1) for (i, digit) in enumerate(reverse(digit_list))])\n",
    "    #Convert the list of digits into an integer by multiplying each digit by the \n",
    "    #next power of 10 and adding them all together. Reverse the array so that\n",
    "    #the first digit in the array gets multiplied by the largest power of 10\n",
    "    #and becomes the leftmost digit in the resulting integer.\n",
    "end\n",
    "\n",
    "function encode_state(state::WinState)\n",
    "    return -1\n",
    "    #Encode a winning state as -1.\n",
    "end\n",
    "\n",
    "function encode_state(state::LossState)\n",
    "     return -2\n",
    "    #Encode a losing state as -2.\n",
    "end\n",
    "\n",
    "\n",
    "function decode_state(coding::Int64)\n",
    "    \"\"\"This function decodes a game state that has been\n",
    "    encoded as an integer and returns a `DuelingState` structure.\"\"\"\n",
    "    if coding < 0\n",
    "        return [WinState(),LossState()][abs(coding)]\n",
    "    end\n",
    "    digit_list = reverse(digits(coding))\n",
    "    #Convert the encoding into an array of digits, and reverse it so\n",
    "    #that the leftmost digit is the first entry in the array.\n",
    "    state = DuelingState(\n",
    "        digit_list[1],\n",
    "        (\n",
    "            guard=digit_list[2], \n",
    "            rush=digit_list[3], \n",
    "            dodge=digit_list[4], \n",
    "            strike=digit_list[5], \n",
    "            punish=digit_list[6]\n",
    "        ),\n",
    "        (\n",
    "            hp=digit_list[7],\n",
    "            exhausted=Bool(digit_list[8]),\n",
    "            feinted=Bool(digit_list[9])\n",
    "        ),\n",
    "        (\n",
    "            hp=digit_list[10], \n",
    "            hand_size=digit_list[11],\n",
    "            exhausted=Bool(digit_list[12]),\n",
    "            feinted=Bool(digit_list[13])\n",
    "        ),\n",
    "        (\n",
    "            guard=digit_list[14], \n",
    "            rush=digit_list[15], \n",
    "            dodge=digit_list[16], \n",
    "            strike=digit_list[17], \n",
    "            punish=digit_list[18]\n",
    "        )\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7c74e",
   "metadata": {},
   "source": [
    "Just in case, we will define a dispatch of the `==` operator to compare a pair of `PunishState`s. Turns out that we can easily compare a pair of `PunishState` objects by comparing their integer encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3f0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.:(==)(x::PunishState, y::PunishState) = encode_state(x) == encode_state(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f9631",
   "metadata": {},
   "source": [
    "# Representing Actions <a id=\"actions\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad938843",
   "metadata": {},
   "source": [
    "Now, we need to decicde how we will represent actions. There are no more than 10 unique actions that can be taken in the game of Punish: play one of any of the 5 card types, or feint with one of those cards. I think that it will be most convenient to work with actions that are represented as `(card::Symbol, feint::Bool)` pairs. However, for memory efficiency, we will encode these pairs as a two-digit integer in which the first digit identifies the card and the second digit indicates whether or not that card is used for a feint.\n",
    "\n",
    "Importantly, if a player is exhausted, then they may not take any actions. We will use the `(:rest,:false)` tuple for this \"action\", which we will encode numerically as `90`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614ede41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode_action (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode_action(action::Tuple{Symbol,Bool})\n",
    "    \"\"\"This function takes an action represented as a (card, is_feint) tuple and \n",
    "    encodes it as an integer in which the first digit represents the card used and\n",
    "    the second represents whether or not the action is a feint.\n",
    "    \"\"\"\n",
    "    card_identifiers = (guard=1,rush=2,dodge=3,strike=4,punish=5,rest=9)\n",
    "    #Encode cards as their priority +1; encode a \"rest\"/the nothing card as a 9.\n",
    "    return card_identifiers[action[1]]*10+Int(action[2])\n",
    "end\n",
    "\n",
    "function decode_action(coding::Int)\n",
    "    \"\"\"This function decodes an action encoded as an integer back into a (card,is_feint) tuple.\"\"\"\n",
    "    card_identifiers = Dict(1=>:guard,2=>:rush,3=>:dodge,4=>:strike,5=>:punish,9=>:rest)\n",
    "    card = card_identifiers[digits(coding)[2]]\n",
    "    feint = Bool(digits(coding)[1])\n",
    "    return (card,feint)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a046ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "possible_actions (generic function with 4 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function possible_actions(state::DuelingState)\n",
    "    \"\"\"This function returns an encoded list of the possible actions\n",
    "    that can be taken by the player from a given state.\"\"\"\n",
    "    actions = []\n",
    "    if (state.status.exhausted) || (state.breath==5)\n",
    "        return [encode_action((:rest,false))]\n",
    "        #When the player is exhausted, the only action they can take\n",
    "        #is rest (which by definition does not involve a feint).\n",
    "        #Use Rest without reint as a sentinel action for intermediary\n",
    "        #\"Breath 5\" states.\n",
    "    end\n",
    "    \n",
    "    for (card,count) in pairs(state.hand)\n",
    "        if count == 0 \n",
    "            continue\n",
    "        else\n",
    "            push!(actions,encode_action((card,false)))\n",
    "            #The player can play any card that they hold at least 1 of.\n",
    "            if !state.status.feinted\n",
    "                push!(actions,encode_action((card,true)))\n",
    "                #If they have not already feinted this breath, they also \n",
    "                #have the option of discarding that card to perform a feint.\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return actions\n",
    "end\n",
    "\n",
    "function possible_actions(state::Int)\n",
    "     return possible_actions(decode_state(state))\n",
    "end\n",
    "\n",
    "function possible_actions(state::WinState)\n",
    "     return [90]\n",
    "end\n",
    "\n",
    "function possible_actions(state::LossState)\n",
    "     return [90]\n",
    "end\n",
    "#Use Rest without feint as a sentinel action for Win/Loss states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dd9d7",
   "metadata": {},
   "source": [
    "# Simulating the Game Environment <a id=\"environment\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463ec0e",
   "metadata": {},
   "source": [
    "In this section, we will create the functions that implement our representation of the overall game environment. In RL problems, the environment is typically modeled as a Markov Decision Process (MDP) — a model in which at any state, an agent can choose from some set of actions that will influence the probabilities of transitioning to different states, and the set of possible actions and transition probabilities from any given state are fixed. In other words, the actions and transition probabilities are a state function rather than a path function.\n",
    "\n",
    "Treating PUNISH as an MDP is a bit of an oversimplification because throughout the course of a Measure, you gain information about what is in the opponent's hand. The most clear example is that if you play a Punish against a Dodge and the enemy does not follow up with a Punish, you know that they do not have a Punish in hand. Additionally, if both players feint and do not use Punish until the last measure, you know with certainty the single card that is in the enemy's hand.\n",
    "\n",
    "Now, if I were to include this information in the game state, there would be less bias in treating the game as an MDP. However, I made the command decision that the benefit of including this information is outweighed by the increase in the size of the state space that would result from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1436b805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ΔHP (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ΔHP(cards::NamedTuple{(:player,:enemy),Tuple{Symbol,Symbol}})\n",
    "    \"\"\"This function computes the change in HP incurred by each player\n",
    "    when a pair of cards is played. Priority is not taken into account here.\"\"\"\n",
    "    if cards.player == cards.enemy\n",
    "        return (player=0, enemy=0)\n",
    "        #No damage is dealt in a clash. \n",
    "    end\n",
    "    \n",
    "    damage_map = Dict(:rush=>1,:strike=>2,:punish=>3)\n",
    "    player_Δ = -get(damage_map,cards.enemy,0) *\n",
    "        #Base damage.\n",
    "        (cards.player==:dodge ? cards.enemy==:rush : 1) +\n",
    "        #Dodging reduces base damage from non-Rush attacks to 0.\n",
    "        (cards.player==:guard ? Int(cards.enemy in keys(damage_map)) : 0)\n",
    "        #Guarding reduces damage from any attacks by 1.\n",
    "    enemy_Δ = -get(damage_map,cards.player,0) *\n",
    "        #Base damage.\n",
    "        (cards.enemy==:dodge ? cards.player==:rush : 1) +\n",
    "        #Dodging reduces base damage from non-Rush attacks to 0.\n",
    "        (cards.enemy==:guard ? Int(cards.player in keys(damage_map)) : 0)\n",
    "        #Guarding reduces damage from any attacks by 1.\n",
    "    \n",
    "    return (player=player_Δ, enemy=enemy_Δ)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae57d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol, Int64} with 6 entries:\n",
       "  :strike => 3\n",
       "  :dodge  => 2\n",
       "  :guard  => 0\n",
       "  :punish => 4\n",
       "  :rest   => -1\n",
       "  :rush   => 1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority = Dict(\n",
    "    :rest => -1,\n",
    "    :guard => 0,\n",
    "    :rush => 1,\n",
    "    :dodge => 2,\n",
    "    :strike => 3,\n",
    "    :punish => 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b822eda",
   "metadata": {},
   "source": [
    "Before we continue, there is a quirk we must introduce into our model. After the fourth Breath of a Measure is resolved, if both players remain above 0 HP, the deck and the discard pile are shuffled together and are used to deal both players back up to 5 cards. This means that the successors of a Breath 4 state will be Breath 1 states (and Win/Loss states). \n",
    "\n",
    "In later sections, we compute transition probabilities for each (state,action) pair. Due to the overwhelmingly large cardinality of the set of possible Breath 4 states (>100k), we found it to be prohibitively inefficient to compute all possible re-dealings for that many Breath 4 states. To remedy this, we will use intermediary \"Breath 5\" states to represent game states after the actions taken in a fourth Breath are resolved but before the cards are redealt. Since status effects are cleared and players heal 1 HP (to a max of 3) between Measures, we will apply this in between Breath 4 and Breath 5 states, which will result in a smaller number of possible Breath 5 states over which redealing probabilities must be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfc8ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "breath (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function breath(\n",
    "    state::DuelingState,\n",
    "    picks::NamedTuple{(:player,:enemy),Tuple{Symbol,Symbol}},\n",
    "    feints::NamedTuple,\n",
    "    )\n",
    "    \"\"\"This function takes complete information about one Breath (i.e. the input state,\n",
    "    as well as both players' picked cards and the results of any feints) and returns the\n",
    "    successor state. \n",
    "    \n",
    "    This function is designed to be deterministic and single-valued. To that end, successors\n",
    "    of a fourth breath are returned as a single placeholder whose `breath` field is set to 5;\n",
    "    we will use a different function to compute all possible re-deals for the start of a new\n",
    "    measure.\n",
    "    \"\"\"\n",
    "    plays = (\n",
    "        player=isnothing(feints.player) ? picks.player : feints.player, \n",
    "        enemy=isnothing(feints.enemy) ? picks.enemy : feints.enemy\n",
    "    )\n",
    "    #Track the actual cards that are played \n",
    "    #(i.e. if feinting, the drawn card; otherwise, the picked card).\n",
    "    \n",
    "    successor = DuelingState(\n",
    "        state.breath + 1,\n",
    "        (; [(card,count-Int(card==picks.player)) for (card,count) in pairs(state.hand)]...),\n",
    "        #Whichever card was picked is removed from the hand.\n",
    "        (\n",
    "            hp = state.status.hp+ΔHP(plays).player,\n",
    "            #Compute the change in HP and apply it to the HP total.\n",
    "            exhausted = (plays.player==:punish),\n",
    "            #If the player played Punish this breath they become exhausted.\n",
    "            feinted = (!isnothing(feints.player)||state.status.feinted)\n",
    "            #The player has feinted if they feinted this breath or in a previous breath.\n",
    "        ),\n",
    "        (\n",
    "            hp = state.enemy.hp+ΔHP(plays).enemy, \n",
    "            hand_size = (state.enemy.hand_size-Int(plays.enemy!=:rest)),\n",
    "            #The enemy's hand size decreases by 1 unless they rested after a Punish.\n",
    "            exhausted= (plays.enemy==:punish),\n",
    "            feinted = (!isnothing(feints.enemy)||state.enemy.feinted)\n",
    "        ),\n",
    "        (; [(card,count+sum([picks...,feints...].==card)) \n",
    "            for (card,count) in pairs(state.discard)]...),\n",
    "        #For each card type in the discard pile, increment the count for every\n",
    "        #pick and every feint that matched that type this breath.  \n",
    "    )\n",
    "    \n",
    "    if successor.status.hp <= 0\n",
    "        if successor.enemy.hp <= 0\n",
    "            return priority[plays.player] < priority[plays.enemy] ? WinState() : LossState()\n",
    "            #If both players would be reduced to nonpositive HP this breath, the winner\n",
    "            #is determined by card priority.\n",
    "        else\n",
    "            return LossState()\n",
    "            #If just the player would be reduced to nonpositive HP, it is a loss.\n",
    "        end\n",
    "    elseif successor.enemy.hp <= 0\n",
    "        return WinState()\n",
    "        #If just the enemy would be reduced to nonpositive HP, it is a win.\n",
    "    else\n",
    "        if successor.breath == 5\n",
    "            successor = DuelingState(\n",
    "                successor.breath,\n",
    "                successor.hand,\n",
    "                (hp=min(3,successor.status.hp+1),exhausted=false,feinted=false),\n",
    "                (hp=min(3,successor.enemy.hp+1),hand_size=successor.enemy.hand_size,exhausted=false,feinted=false),\n",
    "                successor.discard\n",
    "            )\n",
    "            #When entering a Breath 5 state, both players heal 1 HP (to a max of 3),\n",
    "            #and status effects (feinted/exhausted) are cleared.\n",
    "        end\n",
    "        \n",
    "        return successor\n",
    "        #If both the player and the enemy have positive HP, then the duel continues.\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81cf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redeal (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function redeal(state::DuelingState)\n",
    "    \"\"\"This function takes an end-of-measure (\"Breath 5\") state and returns a\n",
    "    dict mapping each possible beginning-of-new-measure state to its\n",
    "    probability of occuring based on a random redealing of cards.\"\"\"\n",
    "    successors = []\n",
    "    for draws in combinations(\n",
    "        vcat([repeat([card],count) for (card, count) in pairs(state.discard)]...),\n",
    "        5-sum(state.hand)\n",
    "    )\n",
    "        #Randomly draw a number of cards equal to the difference between 5 and your \n",
    "        #ending hand size from the cards visible in the discard pile.\n",
    "        \n",
    "        hand = (; [(card,count+sum(draws.==card)) for (card,count) in pairs(state.hand)]...)\n",
    "        #Add the drawn cards to the cards remaining in your hand at the end of the Breath.\n",
    "        \n",
    "        for discards in combinations(\n",
    "            vcat([repeat([card],count-sum(draws.==card)) for (card, count) in pairs(state.discard)]...),\n",
    "            3\n",
    "        )\n",
    "            #Randomly choose 3 cards to go face-up into the discard pile from the cards \n",
    "            #that remain after you've made your draws.\n",
    "            discard = (; [(card,sum(discards.==card)) for (card,count) in pairs(state.discard)]...)\n",
    "            #These three cards replace the old discard pile. The rest of the cards fill the \n",
    "            #enemy's hand and the face-down deck.\n",
    "            \n",
    "            successor = DuelingState(\n",
    "                1, #The first breath of a new measure.\n",
    "                hand,\n",
    "                (hp=state.status.hp,exhausted=false,feinted=false),\n",
    "                #Between measures, each player heals 1HP up to a maximum of 3,\n",
    "                #and status effects are removed.\n",
    "                (hp=state.enemy.hp,hand_size=5,exhausted=false,feinted=false),\n",
    "                discard\n",
    "            )\n",
    "            \n",
    "            push!(successors,encode_state(successor))\n",
    "        end\n",
    "    end\n",
    "    return proportionmap(successors)\n",
    "end\n",
    "\n",
    "\n",
    "function redeal(state::Int)\n",
    "    return redeal(decode_state(state))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ea5c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enemy_states (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function enemy_states(state::DuelingState)\n",
    "    \"\"\"This function takes a DuelingState based on the player's incomplete\n",
    "    information and returns a dict mapping each possible state of the enemy's\n",
    "    incomplete information to a probability based on card counts.\"\"\"\n",
    "    \n",
    "    enemy_states = []\n",
    "    \n",
    "    for enemy_hand in combinations(\n",
    "        vcat([repeat([card],3-state.hand[card]-state.discard[card]) for card in keys(state.discard)]...),\n",
    "        state.enemy.hand_size\n",
    "    )\n",
    "    #The enemy's hand could be any member of the set of all possible combinations of n cards chosen\n",
    "    #from whichever cards the player cannot see, where n is the enemy's hand size. \n",
    "        enemy_state = DuelingState(\n",
    "            state.breath,\n",
    "            (; [(card,sum(enemy_hand.==card)) for card in [:guard,:rush,:dodge,:strike,:punish]]...),\n",
    "            (hp=state.enemy.hp, exhausted=state.enemy.exhausted, feinted=state.enemy.feinted),\n",
    "            (\n",
    "                hp=state.status.hp,\n",
    "                hand_size=sum(state.hand),\n",
    "                exhausted=state.status.exhausted,\n",
    "                feinted=state.status.feinted\n",
    "            ),\n",
    "            state.discard\n",
    "        )\n",
    "        #Breath number, discard, and statuses are all common information. \n",
    "        push!(enemy_states,encode_state(enemy_state))\n",
    "    end\n",
    "    return proportionmap(enemy_states)\n",
    "    #Return normalized value counts of the possible enemy states.\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1f74db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transitionmap (generic function with 4 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transitionmap(state::DuelingState,action::Tuple{Symbol,Bool}; empirical_strategies=Dict())\n",
    "    \"\"\"This function takes a `DuelingState` and an action tuple and\n",
    "    returns a dict mapping all possible successor states to the probability of \n",
    "    that successor resulting from taking the given action from the given state, i.e.\n",
    "    a dict of (successor, transition probability) pairs.\n",
    "    \n",
    "    The `empirical_strategies` keyword argument allows you to pass in a dictionary of \n",
    "    (state, action probability map) pairs corresponding to the enemy's empirically observed\n",
    "    mixed strategies (probability distribution over possible actions). For any states\n",
    "    not in the dictionary, a uniform distribution over all possible actions will be assumed.\n",
    "    \"\"\"\n",
    "    transitions = Dict{Int64,Float64}()\n",
    "    for (enemy_state,p_enemy_state) in enemy_states(state)\n",
    "        #Loop over all possible enemy hands. \n",
    "        enemy_actions = get(\n",
    "            empirical_strategies,\n",
    "            enemy_state,\n",
    "            Dict(enemy_action=>1/length(possible_actions(enemy_state)) \n",
    "                for enemy_action in possible_actions(enemy_state) )\n",
    "        )\n",
    "        #Look up the enemy's state in the empirical strategies dict. If the entry is missing,\n",
    "        #assume a uniform mixed strategy (equal probability of all possible actions).\n",
    "        \n",
    "        decoded_enemy_state = decode_state(enemy_state)\n",
    "        \n",
    "        for (enemy_action,p_enemy_action) in enemy_actions\n",
    "            #Loop over all possible enemy actions.\n",
    "            \n",
    "            deck = vcat([repeat([card],3-(\n",
    "                        state.hand[card]+\n",
    "                        decoded_enemy_state.hand[card]+\n",
    "                        state.discard[card]\n",
    "                        ))\n",
    "                    for card in keys(state.discard)]...)\n",
    "            #With a fixed enemy hand, we have certainty about which cards are in the deck.\n",
    "             \n",
    "            feints = [(player=pf,enemy=ef) for (pf,ef) in zip(\n",
    "                (action[2] ? deck : repeat([nothing],length(deck))),\n",
    "                (decode_action(enemy_action)[2] ? reverse(deck) : repeat([nothing],length(deck))),\n",
    "            )]\n",
    "            #Generate the set of possible outcomes of the feints taken.\n",
    "            if length(feints) == 0\n",
    "                feints = [(player=nothing,enemy=nothing)]\n",
    "            end\n",
    "            \n",
    "            for (feint, p_feint) in proportionmap(feints)\n",
    "                #Loop over the distinct feint outcomces. \n",
    "                picks = (player=action[1],enemy=decode_action(enemy_action)[1])\n",
    "                if state.breath != 5\n",
    "                    successors = Dict(\n",
    "                        encode_state(breath(state,picks,feint))=>1\n",
    "                    )\n",
    "                    #Simulate a Breath using the fixed player and enemy actions and feint outcomes.\n",
    "                else\n",
    "                    successors = redeal(state) \n",
    "                    #If the result is a Breath 5 state, then generate the possible redeals\n",
    "                    #and their probabilities. Otherwise, there is a single successor that\n",
    "                    #occurs with probability 1. \n",
    "                end\n",
    "                for (successor,p_redealt) in successors\n",
    "                    p = p_enemy_state * p_enemy_action * p_feint * p_redealt\n",
    "                    #Multiply all the conditional probabilities to get the overall\n",
    "                    #probability of the successor resulting from the player taking\n",
    "                    #the given action from the given state.\n",
    "                    if successor in keys(transitions)\n",
    "                        transitions[successor] += p\n",
    "                    else\n",
    "                        transitions[successor] = p\n",
    "                    end\n",
    "                    #If this successor has already been mapped out, add the \n",
    "                    #probability from this path to it.\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return transitions\n",
    "end\n",
    "\n",
    "\n",
    "function transitionmap(state::WinState,action::Tuple{Symbol,Bool};kwargs...)\n",
    "    return Dict(-1=>1)\n",
    "end\n",
    "function transitionmap(state::LossState,action::Tuple{Symbol,Bool};kwargs...)\n",
    "    return Dict(-2=>1)\n",
    "end\n",
    "#Win/Loss are \"absorbing states\"; when the game is over, it's over forever.\n",
    "\n",
    "function transitionmap(state::Int,action::Int; kwargs...)\n",
    "     return transitionmap(decode_state(state),decode_action(action);kwargs...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf5d0d",
   "metadata": {},
   "source": [
    "# Enumerating State Space <a id=\"state-space\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38b15a",
   "metadata": {},
   "source": [
    "The next thing we have to do as we lay the groundwork is to generate the set of all possible game states. The following facts restrict the state space from the simple Cartesian product of all possible values of each aspect of the game state:\n",
    "- Neither player's HP can be 0 in a non-end state\n",
    "- Neither player's HP can be less than 2 in a Breath 1 state\n",
    "- Neither player can already be exhausted in a Breath 1 state\n",
    "- Neither player can have already feinted in a Breath 1 state\n",
    "- A player's hand size must be 5-(breath#-1) or 5-(breath#-2); the latter can only be true if there is a Punish in the discard pile for that player\n",
    "- Likewise, the number of cards in the discard pile must be 2*(breath#)+(# of feints)-(# of punishes played), where the # of Punishes played is a variable that cannot exceed 2, and must be at least 1 if there are three Punish cards in the discard\n",
    "- For each card type, the sum of the number in hand and the number showing in the discard pile cannot exceed 3.\n",
    "- A player cannot have an HP of 1 unless \n",
    "    - A Strike was used against them (guarded if they started with 2 HP and unguarded if they started with 1)\n",
    "    - A Rush was used against them and not guarded\n",
    "    - A Punish was used against them and guarded \n",
    "\n",
    "It is clear to see that the state space's intension cannot be stated simply. The last rule we have listed in particular shows that when we look into how the cards played affect the possible status conditions, things get pretty complicated.\n",
    "\n",
    "There might be an easier way to generate the set of possible states. The intension of the set of possible Breath 1 states is much simpler than that of the set of all possible states. If we enumerate all possible Breath 1 states, we can simply use the functions we defined in the previous section to generate the set of all possible successor states, i.e. the set of all possible Breath 2 states. Then, we rinse and repeat to get the Breath 3 and Breath 4 states. This is certainly not the most efficient way to go about this, but we only have to do it once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740330c4",
   "metadata": {},
   "source": [
    "We'll start out by generating the possible starting (Measure 1 Breath 1) states. Since both players start with 3 HP, 5 cards in hand, and no status conditions, this boils down to the set of possible combinations of hands and discard piles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d153daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2150 distinct starting states\n"
     ]
    }
   ],
   "source": [
    "if CONFIG[\"state-space\"][\"generate\"]\n",
    "    \n",
    "STARTING_STATES = Set{Int64}()\n",
    "hands = Set{NamedTuple}()\n",
    "\n",
    "for draws in (combinations(repeat([:guard,:rush,:dodge,:strike,:punish],3),5))\n",
    "    hand = (; [(card,sum(draws.==card)) for card in [:guard,:rush,:dodge,:strike,:punish]]...)\n",
    "    #Loop over all possible starting hands.\n",
    "    if hand in hands\n",
    "        continue\n",
    "        #The `combinations` function does not account for identical elements, so\n",
    "        #skip any combinations that have already been encountered.\n",
    "    else\n",
    "        push!(hands,hand)\n",
    "        #Track the combinations that have been encountered.\n",
    "    end\n",
    "    \n",
    "    discards = []\n",
    "    for discard_draws in combinations(\n",
    "            vcat([repeat([card],3-count) for (card, count) in pairs(hand)]...),\n",
    "            3\n",
    "        )\n",
    "        #Loop over all possible 3-card discard piles taken from the cards that\n",
    "        #remain after the player's hand has been drawn.\n",
    "        discard = (; [(card,sum(discard_draws.==card)) for card in [:guard,:rush,:dodge,:strike,:punish]]...)\n",
    "        if discard in discards\n",
    "            continue\n",
    "        else\n",
    "            push!(discards,discard)\n",
    "        end\n",
    "        #Likewise, skip discard combinations that have already been encountered.\n",
    "        \n",
    "        starting_state = DuelingState(\n",
    "            1,\n",
    "            hand,\n",
    "            (hp=3, exhausted=false, feinted=false),\n",
    "            (hp=3, hand_size=5, exhausted=false, feinted=false),\n",
    "            discard\n",
    "        )\n",
    "        push!(STARTING_STATES,encode_state(starting_state))\n",
    "         \n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "println(\"$(length(STARTING_STATES)) distinct starting states\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29f0ce",
   "metadata": {},
   "source": [
    "    2150 distinct starting states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22465e",
   "metadata": {},
   "source": [
    "Now, we'll generate all possible Breath 1 states. Still no status effects and 5 cards in both players' hands, but either or both players can have 2 HP on Breath 1 after the first Measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "552c1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breath 1: 8600 distinct states\n"
     ]
    }
   ],
   "source": [
    "if CONFIG[\"state-space\"][\"generate\"]\n",
    "\n",
    "BREATH1_STATES = union(\n",
    "    STARTING_STATES,\n",
    "    STARTING_STATES .- 0_00000_100_0000_00000,\n",
    "    #Player starts with 2HP\n",
    "    STARTING_STATES .- 0_00000_000_1000_00000,\n",
    "    #Enemy starts with 2HP\n",
    "    STARTING_STATES .- 0_00000_100_1000_00000\n",
    "    #Both start with 2HP\n",
    ")\n",
    "println(\"Breath 1: $(length(BREATH1_STATES)) distinct states\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347599f2",
   "metadata": {},
   "source": [
    "    Breath 1: 8600 distinct states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163a9e5",
   "metadata": {},
   "source": [
    "Now, we have to find the Breath 2-5 states. We will do this by using the `transitionmap` function to find all the possible successors from each state in the previous breath. This will inevitably be very inefficient, but it will get the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f19ebb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breath 2: 169495 distinct states\n",
      "Breath 3: 259312 distinct states\n",
      "Breath 4: 212142 distinct states\n",
      "Breath 5: 19341 distinct states\n"
     ]
    }
   ],
   "source": [
    "if CONFIG[\"state-space\"][\"generate\"]\n",
    "\n",
    "STATE_SPACE = Set{Int64}([BREATH1_STATES...])\n",
    "new_states = Set{Int64}([BREATH1_STATES...])\n",
    "\n",
    "for breath in 2:5\n",
    "    successors = Set{Int64}()\n",
    "    for state in new_states\n",
    "        for action in possible_actions(state)\n",
    "            union!(successors, keys(transitionmap(state,action)))\n",
    "            #Loop over all distinct states of the previous breath and \n",
    "            #construct a set of all distinct successors from any state\n",
    "            #in that breath.\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    new_states = successors\n",
    "    println(\"Breath $(breath): $(length(new_states)) distinct states\")\n",
    "    #Print the number of distinct states for each breath number.\n",
    "    \n",
    "    if length(new_states)==0\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    union!(STATE_SPACE,successors)\n",
    "end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d877",
   "metadata": {},
   "source": [
    "    Breath 2: 169495 distinct states\n",
    "    Breath 3: 259312 distinct states\n",
    "    Breath 4: 212142 distinct states\n",
    "    Breath 5: 19341 distinct states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6418e8",
   "metadata": {},
   "source": [
    "This cell took about 20 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc2f2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 668884 distinct states\n"
     ]
    }
   ],
   "source": [
    "if CONFIG[\"state-space\"][\"generate\"]\n",
    "    open(CONFIG[\"state-space\"][\"filepath\"],\"w\") do f\n",
    "        JSON.print(f,STATE_SPACE)\n",
    "    end\n",
    "    #If the state space is to be generated, then save the results.\n",
    "else \n",
    "    STATE_SPACE = Set(JSON.parsefile(CONFIG[\"state-space\"][\"filepath\"],use_mmap=false))\n",
    "    #Otherwise, load from disk. The use_mmap keyword must be set to false, otherwise\n",
    "    #the file will remain open in Julia.\n",
    "end\n",
    "println(\"Total: $(length(STATE_SPACE)) distinct states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d0209",
   "metadata": {},
   "source": [
    "# Modeling Enemy Strategies <a id=\"enemy-strategies\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec64605",
   "metadata": {},
   "source": [
    "One of the biases of our current representation of PUNISH is the assumption that at any given state, the enemy will weight each of their possible acitons equally. In actuality, we expect a human player to (on average) avoid taking actions that will lead to imminent defeat. However, human players lack the computational power to estimate expected rewards by considering anything deeper than the immediate Breath. Additionally, attitudes toward risk-taking will also inform a human player's behavior. This is beyond the scope of what we are able to derive from a strictly logical premise. So, we turn to data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f51ff",
   "metadata": {},
   "source": [
    "## Possible Actions Reweighted by Limited Empirical Strategic Samples (PARLESS) <a id=\"parless\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a376a1",
   "metadata": {},
   "source": [
    "In this section, I present a method for using real game data to reweight the enemy's mixed strategies (i.e. probability distribution over possible actions) which I call **\"Possible Actions Reweighted by Limited Empirical Strategic Samples\" (PARLESS)**. In this method, we will use the empirical distribution of actions taken from a given state by real players as the \"evidence\" term in Bayes' rule to reweight the parameters of a multinomial probability distribution, which we use to represent the mixed strategy. We will use a Dirichlet prior with equal probabilities, as it is the conjugate prior of the multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63964356",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/44494/why-is-the-dirichlet-distribution-the-prior-for-the-multinomial-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a1c52",
   "metadata": {},
   "source": [
    "## PARLESS Augmented With Neural Networks (PAWNN) <a id=\"pawnn\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca74cb",
   "metadata": {},
   "source": [
    "PUNISH has a small yet dedicated player base. However, with hundreds of thousands of game states, we simply do not have empirical strategy data for every possible action. However, we may be able to extrapolate from the states that we do have using a deep neural network model. Here, we will experiment with using neural networks trained on the (state ↦ mixed strategy) set generated by the basic PARLESS method to extrapolate mixed strategies for states for which we lack data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f5b89",
   "metadata": {},
   "source": [
    "# Enumerating State-Action Transition Probabilities <a id=\"transitions\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab7aab",
   "metadata": {},
   "source": [
    "Although we have the `transitionmap` function to generate the transition probabilities for any given state-action pair, it will significantly speed up the value iteration algorithm if we pre-compile the transition probabilities.\n",
    "\n",
    "Note: we will need to generate a separate transition probability map for all three models of enemy strategies (Naïve, PARLESS, and PAWNN).\n",
    "\n",
    "Before we included intermediary \"Breath 5\" states, extrapolation from benchmark experiments indicated that generating transition probabilities for each (state,action) pair would take somewhere around 42 hours. With Breath 5 states between non-terminal Breath 4 results and redeals, this computation takes around a half hour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cbe53",
   "metadata": {},
   "source": [
    "## Naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e1a7",
   "metadata": {},
   "source": [
    "This next cell will compute transition probabilities under the Naïve model of enemy strategies, which is just a fancy way of referring to the assumption that the enemy is equally probable to choose any of their possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9bf8c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Int64, Dict{Int64}}\n"
     ]
    }
   ],
   "source": [
    "if CONFIG[\"transition-probabilities\"][\"naive\"][\"generate\"]\n",
    "    NAIVE_TRANSITIONS = Dict( \n",
    "        state => Dict(\n",
    "            action => transitionmap(state,action) \n",
    "            for action in possible_actions(state)\n",
    "        )\n",
    "        for state in STATE_SPACE\n",
    "    )\n",
    "    open(CONFIG[\"transition-probabilities\"][\"naive\"][\"filepath\"],\"w\") do f\n",
    "        JSON.print(f,NAIVE_TRANSITIONS)\n",
    "    end\n",
    "    #Generate transition map and save results.\n",
    "else \n",
    "    NAIVE_TRANSITIONS = strkeys2int(\n",
    "        JSON.parsefile(CONFIG[\"transition-probabilities\"][\"naive\"][\"filepath\"],use_mmap=false)\n",
    "    )\n",
    "    #Load results from disk.\n",
    "end\n",
    "println(typeof(NAIVE_TRANSITIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2a1f0",
   "metadata": {},
   "source": [
    "This cell took about 35 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd363c3",
   "metadata": {},
   "source": [
    "## PARLESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea73cf",
   "metadata": {},
   "source": [
    "## PAWNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656d2eb",
   "metadata": {},
   "source": [
    "# Value Iteration <a id=\"value-iteration\"></a>\n",
    "### [↑ Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed3dab",
   "metadata": {},
   "source": [
    "In this section, we will implement the value iteration algorithm  (and its accessory functions), which we will use to train our agents. Value iteration is an offline reinforcement learning algorithm which applies dynamic programming to an MDP-representation of a game to generate a \"state-action value\" function (also known as a Q-function), which assigns relative values to taking a given action from a given state for every possible (state,action) pairing. [This video](https://youtu.be/4KGC_3GWuPY) does an excellent job showing how the algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a98c5",
   "metadata": {},
   "source": [
    "Reinforcement learning algorithms require a state-action reward function, which assigns quantitative rewards to each (state,action) pair. If we want to incentivize certain kinds of gameplay, we might customize the values in these functions. However, we will just stick with the basic state-action reward function, in which the reward for taking some action at a given state is just the expected value of the state-rewards of all the successors of that action (i.e. the sum of the state rewards of the successors, weighted by their probabilities of resulting from the action). \n",
    "\n",
    "We will also keep our state reward function pretty simple: a WinState is +1, a LossState is -1, and any DuelingState is 0. If we wanted to incentivize the agent to keep games quick (i.e. penalize it for dragging the game out), we might assign a slight negative value to DuelingStates, but I believe that the inherent risk of playing the game is enough of a driving force to keep things short and sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86924171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_action_reward (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function state_reward(s::Int)\n",
    "    return get(Dict(-2=>-1.0,-1=>+1.0),s,0.0)\n",
    "    #Faster than wasting time decoding the state.\n",
    "    #Note: values must be specified as floats or else\n",
    "    #nested dicts in the Q-function may assume values to be\n",
    "    #integers.\n",
    "end\n",
    "\n",
    "function state_action_reward(state::Int,action::Int,transition_probabilities::Dict)\n",
    "     return sum([p*state_reward(successor) for (successor,p) in transition_probabilities[state][action]])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2833a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize_Q_function (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initialize_Q_function(transition_probabilities)\n",
    "    \"\"\"This function initializes a state-action value function\n",
    "    such that each (state,action) pair maps to its immediate expected reward.\"\"\"\n",
    "    Q_function = Dict(\n",
    "        state => Dict(\n",
    "            action => state_action_reward(state,action,transition_probabilities)\n",
    "            for action in keys(transitions)\n",
    "        )\n",
    "        for (state,transitions) in transition_probabilities\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b4964bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function value_iteration!(Q_function::Dict,transition_probabilities::Dict;discount=0.95) \n",
    "    \n",
    "    greedy_choice = (state -> argmax(Q_function[state]))\n",
    "    #Use a greedy policy, in which we choose the action with the highest state-action value for the\n",
    "    #given state.\n",
    "    V_function = Dict(state => Q_function[state][greedy_choice(state)] for state in STATE_SPACE)\n",
    "    #The state-value function assigns each state the value resulting from taking the \n",
    "    #optimal action from that state.\n",
    "\n",
    "    for (state, action_maps) in transition_probabilities\n",
    "        for (action, successors) in action_maps\n",
    "            immediate_reward = state_action_reward(state,action,transition_probabilities)\n",
    "            successor_rewards = sum([p*V_function[successor] for (successor,p) in successors])\n",
    "            Q_function[state][action] = immediate_reward + discount*successor_rewards\n",
    "            #Update each value of the Q-function so that it is equal to the immediate\n",
    "            #state-action reward plus the discounted sum of the state-values\n",
    "            #of the immediate successor states, weighted by their transition probabilities.\n",
    "        end\n",
    "    end\n",
    "    return Q_function\n",
    "                     \n",
    "end\n",
    "\n",
    "function value_iteration(Q_function::Dict,transition_probabilities::Dict;kwargs...)\n",
    "    return value_iteration!(deepcopy(Q_function)),transition_probabilities;kwargs...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1738287c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q_function_error (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Q_function_error(Q1::Dict,Q2::Dict)\n",
    "    err = 0\n",
    "    for (state,actions) in Q1\n",
    "        for (action,value) in actions\n",
    "            err += (Q1[state][action] - Q2[state][action])^2\n",
    "        end\n",
    "    end\n",
    "    return sqrt(err)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Naive = initialize_Q_function(NAIVE_TRANSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fba11db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: MSE=675.5072319577174\n",
      "Episode 2: MSE=695.8641261949957\n",
      "Episode 3: MSE=699.2449865913027\n",
      "Episode 4: MSE=694.72371994294\n",
      "Episode 5: MSE=676.9236153796564\n",
      "Episode 6: MSE=653.0365248434205\n",
      "Episode 7: MSE=631.1884204834796\n",
      "Episode 8: MSE=607.1770272110942\n",
      "Episode 9: MSE=582.094848347449\n",
      "Episode 10: MSE=556.1208080094059\n"
     ]
    }
   ],
   "source": [
    "for i in 1:CONFIG[\"value-iteration\"][\"naive\"][\"episodes\"]\n",
    "    Q_Naive_old = deepcopy(Q_Naive)\n",
    "    Q_Naive = value_iteration(Q_Naive,NAIVE_TRANSITIONS)\n",
    "    err = Q_function_error(Q_Naive,Q_Naive_old)\n",
    "    println(\"Episode $(i): RMSE=$(err)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "450e552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11: MSE=530.4625931050253\n",
      "Episode 12: MSE=506.2130590288164\n",
      "Episode 13: MSE=482.52330408972006\n",
      "Episode 14: MSE=459.5696114092691\n",
      "Episode 15: MSE=437.32518230651664\n",
      "Episode 16: MSE=415.97065408268776\n",
      "Episode 17: MSE=395.66596574004996\n",
      "Episode 18: MSE=376.242717973588\n",
      "Episode 19: MSE=357.6908337182987\n",
      "Episode 20: MSE=339.98024128334373\n",
      "Episode 21: MSE=323.11393501996395\n",
      "Episode 22: MSE=307.07907540675865\n",
      "Episode 23: MSE=291.82211313170365\n",
      "Episode 24: MSE=277.3136001882871\n",
      "Episode 25: MSE=263.5046058558684\n",
      "Episode 26: MSE=250.37438183428156\n",
      "Episode 27: MSE=237.896804447168\n",
      "Episode 28: MSE=226.04005264060535\n",
      "Episode 29: MSE=214.76758371563767\n",
      "Episode 30: MSE=204.04966222166735\n",
      "Episode 31: MSE=193.86190225970594\n",
      "Episode 32: MSE=184.18271727510154\n",
      "Episode 33: MSE=174.98672470044684\n",
      "Episode 34: MSE=166.24930864414736\n",
      "Episode 35: MSE=157.94654132110998\n",
      "Episode 36: MSE=150.0576705316692\n",
      "Episode 37: MSE=142.5619374525369\n",
      "Episode 38: MSE=135.43995954951887\n",
      "Episode 39: MSE=128.67268138319315\n",
      "Episode 40: MSE=122.24329145479759\n",
      "Episode 41: MSE=116.13517470389267\n",
      "Episode 42: MSE=110.33253115969264\n",
      "Episode 43: MSE=104.81938631321529\n",
      "Episode 44: MSE=99.5809682927342\n",
      "Episode 45: MSE=94.60418882823367\n",
      "Episode 46: MSE=89.87631770872905\n",
      "Episode 47: MSE=85.38449150908642\n",
      "Episode 48: MSE=81.1171060619284\n",
      "Episode 49: MSE=77.06270419907452\n",
      "Episode 50: MSE=73.21076184525087\n",
      "Episode 51: MSE=69.55152780724097\n",
      "Episode 52: MSE=66.07503722624335\n",
      "Episode 53: MSE=62.77237883546506\n",
      "Episode 54: MSE=59.634738104971575\n",
      "Episode 55: MSE=56.6539651736026\n",
      "Episode 56: MSE=53.82198397145356\n",
      "Episode 57: MSE=51.13147914230314\n",
      "Episode 58: MSE=48.57546563306912\n",
      "Episode 59: MSE=46.14736858185456\n",
      "Episode 60: MSE=43.84050890921055\n",
      "Episode 61: MSE=41.648885565035364\n",
      "Episode 62: MSE=39.5668250753026\n",
      "Episode 63: MSE=37.588782128948324\n",
      "Episode 64: MSE=35.70958311760975\n",
      "Episode 65: MSE=33.924422883421\n",
      "Episode 66: MSE=32.22851531422441\n",
      "Episode 67: MSE=30.617375650885396\n",
      "Episode 68: MSE=29.086956601385623\n",
      "Episode 69: MSE=27.63297089859053\n",
      "Episode 70: MSE=26.25156374187841\n",
      "Episode 71: MSE=24.939206252883917\n",
      "Episode 72: MSE=23.69241037786207\n",
      "Episode 73: MSE=22.50793531089205\n",
      "Episode 74: MSE=21.382627902851027\n",
      "Episode 75: MSE=20.313556020359663\n",
      "Episode 76: MSE=19.29792456331006\n",
      "Episode 77: MSE=18.333061286965492\n",
      "Episode 78: MSE=17.41645543482378\n",
      "Episode 79: MSE=16.54567901925269\n",
      "Episode 80: MSE=15.718451987327805\n",
      "Episode 81: MSE=14.932566481570785\n",
      "Episode 82: MSE=14.185956637622189\n",
      "Episode 83: MSE=13.476683244845713\n",
      "Episode 84: MSE=12.80288185600525\n",
      "Episode 85: MSE=12.16277864268733\n",
      "Episode 86: MSE=11.554696517234088\n",
      "Episode 87: MSE=10.977011007799689\n",
      "Episode 88: MSE=10.428196466716766\n",
      "Episode 89: MSE=9.906858730196111\n",
      "Episode 90: MSE=9.411597313597253\n",
      "Episode 91: MSE=8.941074785411383\n",
      "Episode 92: MSE=8.494058482579815\n",
      "Episode 93: MSE=8.069381213878387\n",
      "Episode 94: MSE=7.665931374855199\n",
      "Episode 95: MSE=7.2826511503459965\n",
      "Episode 96: MSE=6.918531023082556\n",
      "Episode 97: MSE=6.572611917332026\n",
      "Episode 98: MSE=6.2439858986103935\n",
      "Episode 99: MSE=5.931791066698133\n",
      "Episode 100: MSE=5.635218374903665\n"
     ]
    }
   ],
   "source": [
    "for i in 11:100\n",
    "    Q_Naive_old = deepcopy(Q_Naive)\n",
    "    Q_Naive = value_iteration(Q_Naive,NAIVE_TRANSITIONS)\n",
    "    err = Q_function_error(Q_Naive,Q_Naive_old)\n",
    "    println(\"Episode $(i): RMSE=$(err)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "730c2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"q_naive.json\",\"w\") do f\n",
    "    JSON.print(f,Q_Naive)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
